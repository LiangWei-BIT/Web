<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Transferring Object: Joint Inference of Container and Human Pose</title>

<meta http-equiv="Content-Language" content="en-us">



<meta content="MSHTML 6.00.6000.16674" name="GENERATOR">

<style type="text/css">

<!--

.style1 {

	font-family: "Times New Roman", Times, serif;

	font-weight: bold;

}

.style2 {font-family: "Times New Roman", Times, serif}

.style7 {font-size: 16px}

-->

</style>


<script type="text/javascript" async="" src="./iccv2017container_files/ga.js.下载"></script><script type="text/javascript" async="" src="./iccv2017container_files/ga.js(1).下载"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22603262-4']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

</head>

<body style="TEXT-ALIGN: center" class=" hasGoogleVoiceExt"><div id="StayFocusd-infobar" style="display:none;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>

<p align="center"><b><font face="Times New Roman" size="6">Transferring Object: Joint Inference of Container and Human Pose</font></b></p>

<div align="center">

<table width="1000" border="0" id="table1">

  <tbody>

  <tr>
    <td><p align="center"><b><font face="Times New Roman" size="5">Hanqing Wang<sup>1</sup>,Wei Liang<sup>1</sup></font></b></p></td>
    <td><p align="center"><b><font face="Times New Roman" size="5">      </font></b></p></td>
    <td><p align="left"><b><font face="Times New Roman" size="5">Lap-Fai Yu<sup>2</sup></font></b></p></td>
  </tr>
  </tbody></table>



<table width="1000" border="0" id="table1">

  <tbody>
  <tr>
    <td><p align="right"><b><font face="Times New Roman" size="4"><sup>1</sup>Beijing Institute of Technology </font></b></p></td>
    <td><p align="center"><b><font face="Times New Roman" size="4"><sup>2</sup>University of Massachusetts Boston</font></b></p></td>

  </tr>
  <tr>   
  </tr>


  </tbody>

</table>


<br>
<br>

<table width="1000" border="0">

<tbody><tr>
<td><div align="center"><img src="./iccv2017container_files/1111.png" alt="" width="1000"></div></td>
</tr>

<tr>
<td><div align="left">
</div></td>
</tr></tbody></table>



<p align="left"><span class="style2"><b><font size="5">Abstract</font></b></span></p>

<p align="left"><span class="style2">
Transferring objects from one place to another place is a very common task performed by humans in daily life. While it is usually intuitive for humans to choose a proper container and an efficient pose to transfer objects, it is non-trivial for computers. In this paper, we propose an approach to jointly infer the container and human pose for performing a transfer task, to allow computers to predict and reason about how humans interact with their physical surroundings with regard to transferring objects given visual input. Given a transfer task, our approach infers a proper container and a carrying pose by minimizing the costs associated with the transfer. In the learning phase, our approach learns how humans make rational choices of containers and poses for transferring different objects. It also learns the physical qualities of transfer tasks (e.g., compatibility between container and containee, energy cost of carrying pose) by a structured learning approach. In the inference phase, given a scanned 3D scene with different containers and the target containees, our approach infers the best container and pose for carrying out the transfer task. Actually, our approach learns the importance of different coefficients in transfer tasks. Our experiments show that people estimate the affordance of the object in transfer task according to the the coefficients like the compatibility of containee and container, the compatibility of container and human pose. From this perspective, many objects can be viewed as a container. Further more, it helps computer to comprehend the pose of people during transfer tasks, which can be used in man-machine cooperation and the danger estimation during labour.
</span></p>

<p align="left" class="style2"><span class="style7"><strong>Index Terms</strong>:  HCI, tool understanding</span></p>

<h2 align="left"><font face="Times New Roman">Publication:</font></h2>

<ul>

  <li>

  <p align="left"><strong><font face="Times New Roman">Transferring Object: Joint Inference of Container and Human Pose</font></strong><font face="Times New Roman"><br>

	<a>Hanqing Wang</a>,
    <a href="http://iitlab.bit.edu.cn/mcislab/~liangwei/">Wei Liang</a>,
    <a href="http://www.cs.umb.edu/~craigyu/">Lap-Fai Yu</a>
	<br> IEEE International Conference on Computer Vision <b>(ICCV 2017)</b>
	<br><a href="iccv2017container_files/iccv2017container.pdf">Paper</a><span class="bodyText STYLE3">, <a href="https://youtu.be/t6x6Nod0CwY">Video</a>
	<br>

	<br>


	</span></font></p><h3 align="left"><font face="Times New Roman"><b><font face="Times New Roman">BibTex:</font></b></font></h3><font face="Times New Roman">


<p align="left"><font face="Times New Roman">@article{Wang2017Transferring,<br>

</font><font face="Times New Roman" color="#000000">

&nbsp;&nbsp;&nbsp;
</font><font face="Times New Roman" color="#000000">title </font><font face="Times New Roman">=
{<font color="#000000">Transferring Object: Joint Inference of Container and Human Pose</font>},<br>

&nbsp;&nbsp;&nbsp; author </font><font face="Times New Roman">= {Wang, Hanqing and Liang, Wei and Yu, Lap-Fai},

<br> &nbsp;&nbsp;&nbsp;

journal = {</font><span lang="EN-US" style="FONT-FAMILY: &quot;Times New Roman&quot;">IEEE International Conference on Computer Vision</span><font face="Times New Roman">},<br>

&nbsp;&nbsp;&nbsp; year = {2017}<br>
&nbsp;&nbsp;&nbsp; publisher = {IEEE}<br>

}</font></p>


<p align="left">

<iframe width="560" height="315" src="https://www.youtube.com/embed/t6x6Nod0CwY" frameborder="0" allowfullscreen></iframe>

</p>
  </font></li></ul><font face="Times New Roman">

<h3 align="left">Acknowledgements:</h3>

<p align="left">

This research is supported by the Joseph P. Healey Research Grant Program provided by the Office of the Vice Provost for Research and Strategic Initiatives & Dean of Graduate Studies of UMass Boston. This research is also supported by the National Science Foundation under award number 1565978. We acknowledge NVIDIA Corporation for graphics card donation.



</p>


<!--<hr align="center" width="100%" size="2">


<h2 align="left" class="style3">An office scene used as an illustrative example of our approach</h2>


<table align="center" width="1000" border="0">
<tbody>
<tr><th><img src="./iccv2017container_files/illustration.png" alt="" width="50%"></th></tr>

</tbody>
</table>

<br>
<br>


<h2 align="left" class="style3">Scenes in our work</h2>


<table align="center" width="1000" border="0">
<tbody>
<tr><th><img src="./iccv2017container_files/scenes.png" alt="" width="100%"></th></tr>

</tbody>
</table>
-->
<br>
<br>


<p align="left">
</p><p align="left">

</p><p></p>

</font></div><font face="Times New Roman">


</font>
</body></html>